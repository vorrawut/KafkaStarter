# Concept

## Schema Registry with Avro & Protobuf - Managing Structured Data

## üéØ Objective

Master structured data management in Kafka using Schema Registry with Avro and Protobuf. Learn schema evolution, compatibility strategies, and type safety for production-grade event-driven systems.

## üîç **The Problem with JSON**

While JSON is human-readable and flexible, it has limitations for production systems:

```kotlin
// JSON - No schema enforcement
val userEvent = """
{
  "userId": 123,           // Should be string, not number!
  "email": "test@",        // Invalid email format
  "timestamp": "invalid",  // Invalid timestamp format
  "newField": "surprise"   // Unexpected field - breaks consumers?
}
"""
```

**JSON Challenges:**
- ‚ùå **No schema validation** - Invalid data propagates
- ‚ùå **No evolution strategy** - Adding fields breaks consumers
- ‚ùå **Large payload size** - Field names repeated in every message
- ‚ùå **No type safety** - Runtime errors instead of compile-time checks

## üõ°Ô∏è **Schema Registry: Structured Data Solution**

Schema Registry provides **centralized schema management** with evolution and compatibility checking.

```mermaid
graph TB
    subgraph "Schema Registry Ecosystem"
        SR[Schema Registry :8081]
        
        subgraph "Producers"
            P1[User Service Producer]
            P2[Order Service Producer]
            P3[Analytics Producer]
        end
        
        subgraph "Consumers"
            C1[Email Service Consumer]
            C2[Audit Service Consumer]
            C3[Analytics Consumer]
        end
        
        subgraph "Kafka Broker"
            T1[user-events]
            T2[order-events]
        end
    end
    
    P1 --> SR
    P2 --> SR
    P3 --> SR
    
    SR --> C1
    SR --> C2
    SR --> C3
    
    P1 --> T1
    P2 --> T2
    P3 --> T2
    
    T1 --> C1
    T1 --> C2
    T2 --> C3
```

**Schema Registry Benefits:**
- ‚úÖ **Schema validation** - Only valid data reaches consumers
- ‚úÖ **Evolution management** - Safe schema changes over time
- ‚úÖ **Type safety** - Compile-time error detection
- ‚úÖ **Compact serialization** - Binary format reduces payload size
- ‚úÖ **Backward/Forward compatibility** - Gradual system updates

## üèóÔ∏è **Apache Avro: Dynamic Schema Evolution**

Avro provides **schema evolution** with **dynamic typing** at runtime.

### Avro Schema Definition

```json
{
  "type": "record",
  "name": "UserRegistered",
  "namespace": "com.learning.events",
  "fields": [
    {
      "name": "userId",
      "type": "string",
      "doc": "Unique identifier for the user"
    },
    {
      "name": "email",
      "type": "string",
      "doc": "User's email address"
    },
    {
      "name": "timestamp",
      "type": "long",
      "logicalType": "timestamp-millis",
      "doc": "Event timestamp in milliseconds"
    },
    {
      "name": "source",
      "type": "string",
      "default": "unknown",
      "doc": "Event source system"
    }
  ]
}
```

### Schema Evolution Example

```json
// Version 1 - Initial schema
{
  "type": "record",
  "name": "UserRegistered",
  "fields": [
    {"name": "userId", "type": "string"},
    {"name": "email", "type": "string"}
  ]
}

// Version 2 - Backward compatible (added optional field)
{
  "type": "record", 
  "name": "UserRegistered",
  "fields": [
    {"name": "userId", "type": "string"},
    {"name": "email", "type": "string"},
    {"name": "firstName", "type": ["null", "string"], "default": null}
  ]
}

// Version 3 - Forward compatible (removed field with default)
{
  "type": "record",
  "name": "UserRegistered", 
  "fields": [
    {"name": "userId", "type": "string"},
    {"name": "email", "type": "string"},
    {"name": "firstName", "type": ["null", "string"], "default": null},
    {"name": "registrationSource", "type": "string", "default": "web"}
  ]
}
```

### Kotlin Avro Integration

```kotlin
// Generated from Avro schema
data class UserRegistered(
    val userId: String,
    val email: String,
    val timestamp: Long,
    val source: String = "unknown"
) {
    companion object {
        val SCHEMA = Schema.Parser().parse("""
            {
              "type": "record",
              "name": "UserRegistered",
              "fields": [
                {"name": "userId", "type": "string"},
                {"name": "email", "type": "string"},
                {"name": "timestamp", "type": "long"},
                {"name": "source", "type": "string", "default": "unknown"}
              ]
            }
        """)
    }
}
```

## üöÄ **Protocol Buffers: High-Performance Schema**

Protobuf provides **strongly-typed**, **high-performance** serialization with **code generation**.

### Protobuf Schema Definition

```protobuf
// user_events.proto
syntax = "proto3";

package com.learning.events;

import "google/protobuf/timestamp.proto";

message UserRegistered {
  string user_id = 1;
  string email = 2;
  google.protobuf.Timestamp timestamp = 3;
  string source = 4;
  
  // Evolution: Add optional fields with higher field numbers
  optional string first_name = 5;
  optional string last_name = 6;
  optional RegistrationSource registration_source = 7;
}

enum RegistrationSource {
  UNKNOWN = 0;
  WEB = 1;
  MOBILE = 2;
  API = 3;
}
```

### Schema Evolution with Protobuf

```protobuf
// Version 1
message UserRegistered {
  string user_id = 1;
  string email = 2;
}

// Version 2 - Backward compatible
message UserRegistered {
  string user_id = 1;
  string email = 2;
  google.protobuf.Timestamp timestamp = 3;  // New field
  optional string source = 4;               // Optional field
}

// Version 3 - Field number reserved for removed fields
message UserRegistered {
  string user_id = 1;
  string email = 2;
  google.protobuf.Timestamp timestamp = 3;
  // reserved 4;  // Remove source field safely
  optional string first_name = 5;
  optional string last_name = 6;
}
```

## üîÑ **Schema Compatibility Strategies**

### Backward Compatibility
**New consumers** can read **old producer** data.

```kotlin
// Old producer sends: {userId: "123", email: "test@test.com"}
// New consumer expects: {userId: "123", email: "test@test.com", firstName: null}
// ‚úÖ Works: New field gets default value
```

### Forward Compatibility
**Old consumers** can read **new producer** data.

```kotlin
// New producer sends: {userId: "123", email: "test@test.com", firstName: "John"}  
// Old consumer expects: {userId: "123", email: "test@test.com"}
// ‚úÖ Works: Unknown fields are ignored
```

### Full Compatibility
**Both directions** work - gradual rollouts possible.

```kotlin
// Supports both old and new versions simultaneously
// Enables zero-downtime deployments
```

### Breaking Changes
**Incompatible changes** require coordination.

```kotlin
// ‚ùå Breaking: Remove required field
// ‚ùå Breaking: Change field type (string -> int)
// ‚ùå Breaking: Rename field without alias
```

## üõ†Ô∏è **Spring Boot Schema Registry Integration**

### Producer Configuration

```kotlin
@Configuration
class AvroProducerConfig {
    
    @Bean
    fun avroProducerFactory(): ProducerFactory<String, SpecificRecord> {
        val props = mapOf<String, Any>(
            ProducerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG to StringSerializer::class.java,
            ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG to KafkaAvroSerializer::class.java,
            AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG to "http://localhost:8081",
            KafkaAvroSerializerConfig.SPECIFIC_AVRO_READER_CONFIG to true
        )
        return DefaultKafkaProducerFactory(props)
    }
    
    @Bean
    fun avroKafkaTemplate(): KafkaTemplate<String, SpecificRecord> {
        return KafkaTemplate(avroProducerFactory())
    }
}
```

### Consumer Configuration

```kotlin
@Configuration
class AvroConsumerConfig {
    
    @Bean
    fun avroConsumerFactory(): ConsumerFactory<String, SpecificRecord> {
        val props = mapOf<String, Any>(
            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG to "localhost:9092",
            ConsumerConfig.GROUP_ID_CONFIG to "avro-consumer-group",
            ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG to StringDeserializer::class.java,
            ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG to KafkaAvroDeserializer::class.java,
            AbstractKafkaSchemaSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG to "http://localhost:8081",
            KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG to true
        )
        return DefaultKafkaConsumerFactory(props)
    }
}
```

## üìä **Schema Management Workflow**

### 1. Schema Development

```bash
# Register new schema
curl -X POST http://localhost:8081/subjects/user-events-value/versions \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{
    "schema": "{\"type\":\"record\",\"name\":\"UserRegistered\",\"fields\":[{\"name\":\"userId\",\"type\":\"string\"}]}"
  }'
```

### 2. Schema Evolution

```bash
# Check compatibility before registering
curl -X POST http://localhost:8081/compatibility/subjects/user-events-value/versions/latest \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{
    "schema": "{\"type\":\"record\",\"name\":\"UserRegistered\",\"fields\":[{\"name\":\"userId\",\"type\":\"string\"},{\"name\":\"email\",\"type\":\"string\"}]}"
  }'

# Response: {"is_compatible": true}
```

### 3. Schema Retrieval

```bash
# Get latest schema
curl http://localhost:8081/subjects/user-events-value/versions/latest

# Get specific version  
curl http://localhost:8081/subjects/user-events-value/versions/1

# List all subjects
curl http://localhost:8081/subjects
```

## üß™ **Hands-On: Producer with Schema Evolution**

```kotlin
@Service
class SchemaAwareProducer(
    private val avroKafkaTemplate: KafkaTemplate<String, SpecificRecord>
) {
    
    fun sendUserRegistered(userId: String, email: String) {
        // Create Avro record
        val userRegistered = UserRegistered.newBuilder()
            .setUserId(userId)
            .setEmail(email)
            .setTimestamp(System.currentTimeMillis())
            .setSource("api")
            .build()
        
        // Send with schema validation
        avroKafkaTemplate.send("user-events", userId, userRegistered)
            .thenAccept { result ->
                logger.info("Avro message sent: partition=${result.recordMetadata.partition()}")
            }
            .exceptionally { failure ->
                logger.error("Schema validation failed", failure)
                null
            }
    }
}
```

## üîç **Schema Registry Monitoring**

### Schema Versions

```bash
# View schema evolution history
curl http://localhost:8081/subjects/user-events-value/versions

# Response: [1, 2, 3, 4]  # Available versions
```

### Compatibility Settings

```bash
# Set compatibility level
curl -X PUT http://localhost:8081/config/user-events-value \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{"compatibility": "BACKWARD"}'

# Check current setting
curl http://localhost:8081/config/user-events-value
```

### Schema Health Check

```kotlin
@Component
class SchemaRegistryHealthIndicator(
    private val schemaRegistryClient: SchemaRegistryClient
) : HealthIndicator {
    
    override fun health(): Health {
        return try {
            val subjects = schemaRegistryClient.allSubjects
            Health.up()
                .withDetail("schema_registry_url", "http://localhost:8081")
                .withDetail("subjects_count", subjects.size)
                .withDetail("subjects", subjects)
                .build()
        } catch (e: Exception) {
            Health.down()
                .withDetail("error", e.message)
                .build()
        }
    }
}
```

## ‚ö†Ô∏è **Common Pitfalls & Solutions**

### 1. **Schema Compatibility Violations**

```kotlin
// ‚ùå Problem: Breaking change
// Old: {"name": "userId", "type": "string"}
// New: {"name": "userId", "type": "int"}  // Type change breaks compatibility

// ‚úÖ Solution: Add new field, deprecate old
// {"name": "userId", "type": "string"}        // Keep old
// {"name": "userIdNum", "type": "int"}        // Add new
```

### 2. **Schema Subject Naming**

```kotlin
// ‚ùå Problem: Inconsistent naming
// "UserEvent", "user_event", "userEvent"

// ‚úÖ Solution: Consistent convention
// "user-events-value", "order-events-key"
```

### 3. **Default Values**

```avro
// ‚ùå Problem: No default for new required field
{"name": "newField", "type": "string"}

// ‚úÖ Solution: Provide default value
{"name": "newField", "type": "string", "default": "unknown"}
```

## üìà **Performance Comparison**

| Format | Size (bytes) | Serialize (ms) | Deserialize (ms) | Schema Evolution |
|--------|--------------|----------------|------------------|------------------|
| **JSON** | 150 | 0.8 | 0.9 | Manual |
| **Avro** | 45 | 0.4 | 0.3 | Automatic |
| **Protobuf** | 35 | 0.2 | 0.2 | Automatic |

**Protobuf Advantages:**
- üöÄ **Fastest serialization/deserialization**
- üì¶ **Smallest payload size**
- üîí **Strong typing with code generation**
- üõ°Ô∏è **Built-in validation**

**Avro Advantages:**
- üîÑ **Dynamic schema resolution**
- üìã **Schema embedded in data**
- üîÄ **Rich schema evolution features**
- üõ†Ô∏è **Better tooling ecosystem**

## ‚úÖ **Best Practices**

### üéØ **Schema Design**
- **Use meaningful names** for fields and types
- **Add documentation** to all fields
- **Plan for evolution** from the start
- **Keep schemas simple** and focused

### üîÑ **Evolution Strategy**
- **Always add default values** for new fields
- **Never remove required fields** without deprecation
- **Use union types** for optional fields: `["null", "string"]`
- **Test compatibility** before deploying

### üõ°Ô∏è **Production Deployment**
- **Set appropriate compatibility levels** per topic
- **Monitor schema registry health** and performance
- **Version schemas semantically** (v1.0.0, v1.1.0)
- **Backup schema registry** data regularly

### üß™ **Testing**
- **Test schema evolution scenarios** in CI/CD
- **Validate producer/consumer compatibility** across versions
- **Load test with realistic schemas** and payloads
- **Monitor serialization performance** in production

## üöÄ **What's Next?**

With structured data mastery complete, you're ready for [Lesson 6: Development Tools](../lesson_6/concept.md), where you'll learn debugging, testing, and monitoring tools for Kafka applications.

---

*Schema Registry transforms Kafka from a simple message broker to a robust data platform. The type safety and evolution capabilities you've learned are essential for production-grade event-driven architectures.*